{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3\n",
    "#### Name - Shruthi Ramu\n",
    "#### ID - 029368922"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Binary Cross Entropy loss function\n",
    "\n",
    "$\n",
    "L = -((y_1ln(\\hat{y_1}) + (1-y_1)ln(1-\\hat{y_1}))\n",
    "$\n",
    "\n",
    "$\n",
    "L = -y_1 \\ln{\\hat{y_1}} - (1-y_1) \\ln(1-\\hat{y_1})\n",
    "$\n",
    "\n",
    "take derivative of $ \\hat{y_1}$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\hat{y_1}} = -y_1 \\frac{\\partial \\ln \\hat{y_1}}{\\partial \\hat{y_1}} - (1-y_1) \\frac{\\partial \\ln (1-\\hat{y_1})}{\\partial \\hat{y_1}}\n",
    "$\n",
    "\n",
    "We know that\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\ln(\\hat{y_1})}{\\partial \\hat{y_1}} = \\frac {1} {\\hat{y_1}}\n",
    "\\\\\n",
    "\\frac{\\partial \\ln (1-\\hat{y_1})}{\\partial \\hat{y_1}} = \\frac {-1} {(1- \\hat{y_1})}\n",
    "$\n",
    "\n",
    "Applying them,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\hat{y_1}} = -y_1 \\ \\frac{1}{\\hat{y_1}} \\ - (1-y_1) \\ (\\frac{-1}{1-\\hat{y_1}}) \n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\hat{y_1}} = \\frac {-y_1 + y_1\\hat{y_1} + \\hat{y_1} - y_1\\hat{y_1}}{\\hat{y_1}(1-\\hat{y_1})}\n",
    "$\n",
    "\n",
    "So, after the evaluation we get,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\hat{y_1}} = \\frac {\\hat{y_1} - y_1} {\\hat{y_1} (1-\\hat{y_1})}\n",
    "$\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "# Derivations:\n",
    "\n",
    "## 1.\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\vec{w}^{(1)}} = \\begin{bmatrix} \\frac{\\partial L}{\\partial w_{11}^1} & \\frac{\\partial L}{\\partial w_{12}^1} \\\\ \\frac{\\partial L}{\\partial w_{21}^1} & \\frac{\\partial L}{\\partial w_{22}^1} \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### 1.a\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{11}^1} = \\frac{\\partial L}{\\partial h_{1}^1}  \\ \\frac{\\partial h_{1}^1}{\\partial a_1^1} \\ \\frac{\\partial a_1^1}{\\partial w_{11}^1}\n",
    "$\n",
    "\n",
    "Then compute,\n",
    "$\n",
    "a_1^1 = w_{11}^1 \\ h_1^0 + w_{21}^1 \\ h_2^0 + b_1^1\n",
    "\\\\\n",
    "\\frac{\\partial a_1^1}{\\partial w_{11}^1} = h_1^0 = x_1\n",
    "$\n",
    "\n",
    "Use hyperbolic tangent\n",
    "$\n",
    "\\frac{\\partial h_{1}^1}{\\partial a_1^1} = \\frac{\\partial \\tanh(h_{1}^1)}{\\partial a_1^1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial h_{1}^1}{\\partial a_1^1} = 1 - tanh^2(h_1^1)\n",
    "$\n",
    "\n",
    "There is a single path\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{1}^1} = \\frac{\\partial L}{\\partial h_{1}^2}  \\ \\frac{\\partial h_{1}^2}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial h_{1}^1}\n",
    "$\n",
    "\n",
    "Then compute,\n",
    "$\n",
    "a_1^2 = w_{11}^2 \\ h_1^1 + w_{21}^2 \\ h_2^1 + b_1^2\n",
    "\\\\\n",
    "\\frac{\\partial a_1^2}{\\partial h_{1}^1} = w_{11}^2\n",
    "$\n",
    "\n",
    "Compute using sigmoid activation function here.\n",
    "$\n",
    "So, \n",
    "h_1^2 = \\sigma{a_1^2}\n",
    "\\\\\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = \\frac{\\partial \\sigma{a_1^2}}{\\partial a_{1}^2}\n",
    "\\\\\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = \\sigma{a_1^2} \\ (1-\\sigma{a_1^2})\n",
    "\\\\\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = h_1^2 \\ (1-h_1^2)\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{1}^1} = \\frac {\\hat{y_1} - y_1} {\\hat{y_1} (1-\\hat{y_1})} \\ (h_1^2(1-h_1^2)) \\ w_{11}^2\n",
    "$\n",
    "\n",
    "We know that,\n",
    "\n",
    "$\n",
    "h_1^2 = \\hat{y_1}\n",
    "$\n",
    "\n",
    "Use the already computed cross entropy function ,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{1}^2} = \\frac{\\partial L}{\\partial \\hat{y_1}} = \\frac{\\hat{y_1} - y_1}{\\hat{y_1}\\ (1-\\hat{y_1})}\n",
    "$\n",
    "\n",
    "Finally, \n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{11}^1} = ({\\hat{y_1} - y_1}) \\ w_{11}^2 \\ (1-tanh^2(h_1^1)) \\ x_1 \n",
    "$\n",
    "\n",
    "-------------------------------\n",
    "### 1.b\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{12}^1} = \\frac{\\partial L}{\\partial h_{2}^1}  \\ \\frac{\\partial h_{2}^1}{\\partial a_2^1} \\ \\frac{\\partial a_2^1}{\\partial w_{12}^1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial a_2^1}{\\partial w_{12}^1} = h_1^0 = x_1\n",
    "$\n",
    "\n",
    "Use hyperbolic tangent\n",
    "$\n",
    "\\frac{\\partial h_{2}^1}{\\partial a_2^1} = 1 - tanh^2(h_2^1)\n",
    "$\n",
    "\n",
    "There is a single path\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{2}^1} = \\frac{\\partial L}{\\partial h_{1}^2}  \\ \\frac{\\partial h_{1}^2}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial h_{2}^1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{2}^1} = \\frac {\\hat{y_1} - y_1} {\\hat{y_1} (1-\\hat{y_1})} \\ (h_1^2(1-h_1^2)) \\ w_{21}^2\n",
    "$\n",
    "\n",
    "We know that,\n",
    "\n",
    "$\n",
    "h_1^2 = \\hat{y_1}\n",
    "$\n",
    "\n",
    "So,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{12}^1} = ({\\hat{y_1} - y_1}) \\ w_{21}^2 \\ (1-tanh^2(h_2^1)) \\ x_1 \n",
    "$\n",
    "\n",
    "-------------------------------\n",
    "### 1.c\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{21}^1} = \\frac{\\partial L}{\\partial h_{1}^1}  \\ \\frac{\\partial h_{1}^1}{\\partial a_1^1} \\ \\frac{\\partial a_1^1}{\\partial w_{21}^1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial a_1^1}{\\partial w_{21}^1} = h_2^0 = x_2\n",
    "$\n",
    "\n",
    "Use hyperbolic tangent\n",
    "$\n",
    "\\frac{\\partial h_{1}^1}{\\partial a_1^1} = 1 - tanh^2(h_1^1)\n",
    "$\n",
    "\n",
    "There is a single path\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{1}^1} = \\frac{\\partial L}{\\partial h_{1}^1}  \\ \\frac{\\partial h_{1}^1}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial h_{1}^1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{1}^1} = \\frac {\\hat{y_1} - y_1} {\\hat{y_1} (1-\\hat{y_1})} \\ (h_1^2(1-h_1^2)) \\ w_{11}^2\n",
    "$\n",
    "\n",
    "We know that,\n",
    "\n",
    "$\n",
    "h_1^2 = \\hat{y_1}\n",
    "$\n",
    "\n",
    "So,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{21}^1} = ({\\hat{y_1} - y_1}) \\ w_{11}^2 \\ (1-tanh^2(h_1^1)) \\ x_2 \n",
    "$\n",
    "\n",
    "-------------------------------\n",
    "### 1.d\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{22}^1} = \\frac{\\partial L}{\\partial h_{2}^1}  \\ \\frac{\\partial h_{2}^1}{\\partial a_2^1} \\ \\frac{\\partial a_2^1}{\\partial w_{22}^1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial a_2^1}{\\partial w_{22}^1} = h_2^0 = x_2\n",
    "$\n",
    "\n",
    "Use hyperbolic tangent\n",
    "$\n",
    "\\frac{\\partial h_{2}^1}{\\partial a_2^1} = 1 - tanh^2(h_2^1)\n",
    "$\n",
    "\n",
    "There is a single path\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{2}^1} = \\frac{\\partial L}{\\partial h_{1}^2}  \\ \\frac{\\partial h_{1}^2}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial h_{2}^1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{2}^1} = \\frac {\\hat{y_1} - y_1} {\\hat{y_1} (1-\\hat{y_1})} \\ (h_1^2(1-h_1^2)) \\ w_{21}^2\n",
    "$\n",
    "\n",
    "We know that,\n",
    "\n",
    "$\n",
    "h_1^2 = \\hat{y_1}\n",
    "$\n",
    "\n",
    "So,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{12}^1} = ({\\hat{y_1} - y_1}) \\ w_{21}^2 \\ (1-tanh^2(h_2^1)) \\ x_2 \n",
    "$\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "## 2.\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\vec{w}^{(2)}} = \\begin{bmatrix} \\frac{\\partial L}{\\partial w_{11}^2} \\\\ \\frac{\\partial L}{\\partial w_{21}^2} \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### 2.a\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{11}^2} = \\frac{\\partial L}{\\partial \\hat{y_{1}}} \\ \\frac{\\partial \\hat{y_{1}}}{\\partial h_{1}^2}  \\ \\frac{\\partial h_{1}^2}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial w_{11}^2}\n",
    "$\n",
    "\n",
    "We know that \n",
    "\n",
    "$\n",
    " \\hat{y_{1}} = h_{1}^2\n",
    "$\n",
    "\n",
    "So, we can say the following,\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\hat{y_{1}}}{\\partial h_{1}^2} = 1\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\hat{y_{1}}} = \\frac{\\partial L}{\\partial h_1^2}\n",
    "$\n",
    "\n",
    "After the replacing them,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{11}^2} = \\frac{\\partial L}{\\partial h_{1}^2}  \\ \\frac{\\partial h_{1}^2}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial w_{11}^2}\n",
    "$\n",
    "\n",
    "Use the already calculated loss function derivation here,\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{1}^2} = \\frac{\\partial L}{\\partial \\hat{y_{1}}} = \\frac{\\partial L}{\\partial \\hat{y_1}} = \\frac {\\hat{y_1} - y_1} {\\hat{y_1} (1-\\hat{y_1})}\n",
    "$\n",
    "\n",
    "Then use the sigmoid function to compute,\n",
    "$\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = \\frac{\\partial \\sigma{(a_{1}^2)}}{\\partial a_1^2}\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = \\frac{ \\sigma{(a_{1}^2)}}{1- \\sigma{(a_1^2)}}\n",
    "$\n",
    "\n",
    "After computing the derivation, we get,\n",
    "$\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = h_1^2(1-h_1^2)\n",
    "$\n",
    "\n",
    "Then compute,\n",
    "$\n",
    "\\frac{\\partial a_1^2}{\\partial w_{11}^2} = h_1^1\n",
    "$\n",
    "\n",
    "We know that,\n",
    "\n",
    "$\n",
    "h_1^2 = \\hat{y_1}\n",
    "$\n",
    "\n",
    "So, apply all the above computed values here to get,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{11}^2} = ({\\hat{y_1} - y_1}) \\ h_1^1 \n",
    "$\n",
    "\n",
    "-------------------------------\n",
    "### 2.b\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{21}^2} = \\frac{\\partial L}{\\partial \\hat{y_{1}}} \\ \\frac{\\partial \\hat{y_{1}}}{\\partial h_{1}^2}  \\ \\frac{\\partial h_{1}^2}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial w_{21}^2}\n",
    "$\n",
    "\n",
    "We know that \n",
    "\n",
    "$\n",
    " \\hat{y_{1}} = h_{1}^2\n",
    "$\n",
    "\n",
    "So, we can say the following,\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\hat{y_{1}}}{\\partial h_{1}^2} = 1\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\hat{y_{1}}} = \\frac{\\partial L}{\\partial h_1^2}\n",
    "$\n",
    "\n",
    "After the replacing them,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{21}^2} = \\frac{\\partial L}{\\partial h_{1}^2}  \\ \\frac{\\partial h_{1}^2}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial w_{21}^2}\n",
    "$\n",
    "\n",
    "Use the already calculated loss function derivation here,\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{1}^2} = \\frac{\\partial L}{\\partial \\hat{y_{1}}} = \\frac{\\partial L}{\\partial \\hat{y_1}} = \\frac {\\hat{y_1} - y_1} {\\hat{y_1} (1-\\hat{y_1})}\n",
    "$\n",
    "\n",
    "Then use the sigmoid function to compute,\n",
    "$\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = \\frac{\\partial \\sigma{(a_{1}^2)}}{\\partial a_1^2}\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = \\frac{ \\sigma{(a_{1}^2)}}{1- \\sigma{(a_1^2)}}\n",
    "$\n",
    "\n",
    "After computing the derivation, we get,\n",
    "$\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = h_1^2(1-h_1^2)\n",
    "$\n",
    "\n",
    "Then compute,\n",
    "$\n",
    "\\frac{\\partial a_1^2}{\\partial w_{21}^2} = h_2^1\n",
    "$\n",
    "\n",
    "We know that,\n",
    "\n",
    "$\n",
    "h_1^2 = \\hat{y_1}\n",
    "$\n",
    "\n",
    "So, apply all the above computed values here to get,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w_{21}^2} = ({\\hat{y_1} - y_1}) \\ h_2^1 \n",
    "$\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "## 3.\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\vec{b}^{(1)}} = \\begin{bmatrix} \\frac{\\partial L}{\\partial b_{1}^1} \\\\ \\frac{\\partial L}{\\partial b_{2}^1} \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### 3.a\n",
    "$\n",
    "\\frac{\\partial L}{\\partial b_{1}^1} = \\frac{\\partial L}{\\partial h_1^1} \\  \\frac{\\partial h_{1}^1}{\\partial a_1^1} \\ \\frac{\\partial a_1^1}{\\partial b_{1}^1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial a_1^1}{\\partial b_{1}^1} = 1\n",
    "$\n",
    "\n",
    "Use the hyperbolic tangent function here,\n",
    "$\n",
    "\\frac{\\partial h_1^1}{\\partial a_{1}^1} = 1 - tanh^2(h_1^1)\n",
    "$\n",
    "\n",
    "\n",
    "There is a single path\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{1}^1} = \\frac{\\partial L}{\\partial h_{1}^2}  \\ \\frac{\\partial h_{1}^2}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial h_{1}^1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{1}^1} = \\frac {\\hat{y_1} - y_1} {\\hat{y_1} (1-\\hat{y_1})} \\ (h_1^2(1-h_1^2)) \\ w_{11}^2\n",
    "$\n",
    "\n",
    "We know that,\n",
    "\n",
    "$\n",
    "h_1^2 = \\hat{y_1}\n",
    "$\n",
    "\n",
    "So, apply all the above computed values here to get,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial b_{1}^1} = ({\\hat{y_1} - y_1}) \\ w_{11}^2 \\ (1 - tanh^2(h_1^1))\n",
    "$\n",
    "\n",
    "-------------------------------\n",
    "### 3.b\n",
    "$\n",
    "\\frac{\\partial L}{\\partial b_{2}^1} = \\frac{\\partial L}{\\partial h_2^1} \\  \\frac{\\partial h_{2}^1}{\\partial a_2^1} \\ \\frac{\\partial a_2^1}{\\partial b_{2}^1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial a_2^1}{\\partial b_{2}^1} = 1\n",
    "$\n",
    "\n",
    "Use the hyperbolic tangent function here,\n",
    "$\n",
    "\\frac{\\partial h_2^1}{\\partial a_{2}^1} = 1 - tanh^2(h_2^1)\n",
    "$\n",
    "\n",
    "\n",
    "There is a single path\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{2}^1} = \\frac{\\partial L}{\\partial h_{1}^2}  \\ \\frac{\\partial h_{1}^2}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial h_{2}^1}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{1}^1} = \\frac {\\hat{y_1} - y_1} {\\hat{y_1} (1-\\hat{y_1})} \\ (h_1^2(1-h_1^2)) \\ w_{21}^2\n",
    "$\n",
    "\n",
    "We know that,\n",
    "\n",
    "$\n",
    "h_1^2 = \\hat{y_1}\n",
    "$\n",
    "\n",
    "So, apply all the above computed values here to get,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial b_{1}^1} = ({\\hat{y_1} - y_1}) \\ w_{21}^2 \\ (1 - tanh^2(h_2^1))\n",
    "$\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "## 4.\n",
    "$\n",
    "\\frac{\\partial L}{\\partial {b}^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y_{1}}} \\ \\frac{\\partial \\hat{y_{1}}}{\\partial h_{1}^2}  \\ \\frac{\\partial h_{1}^2}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial b^2}\n",
    "$\n",
    "\n",
    "We know that \n",
    "\n",
    "$\n",
    " \\hat{y_{1}} = h_{1}^2\n",
    "$\n",
    "\n",
    "So, we can say the following,\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\hat{y_{1}}}{\\partial h_{1}^2} = 1\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\hat{y_{1}}} = \\frac{\\partial L}{\\partial h_1^2}\n",
    "$\n",
    "\n",
    "After the replacing them,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial b^2} = \\frac{\\partial L}{\\partial h_{1}^2}  \\ \\frac{\\partial h_{1}^2}{\\partial a_1^2} \\ \\frac{\\partial a_1^2}{\\partial b^2}\n",
    "$\n",
    "\n",
    "Use the already calculated loss function derivation here,\n",
    "$\n",
    "\\frac{\\partial L}{\\partial h_{1}^2} = \\frac{\\partial L}{\\partial \\hat{y_{1}}} = \\frac{\\partial L}{\\partial \\hat{y_1}} = \\frac {\\hat{y_1} - y_1} {\\hat{y_1} (1-\\hat{y_1})}\n",
    "$\n",
    "\n",
    "Then use the sigmoid function to compute,\n",
    "$\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = \\frac{\\partial \\sigma{(a_{1}^2)}}{\\partial a_1^2}\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = \\frac{ \\sigma{(a_{1}^2)}}{1- \\sigma{(a_1^2)}}\n",
    "$\n",
    "\n",
    "After computing the derivation, we get,\n",
    "$\n",
    "\\frac{\\partial h_{1}^2}{\\partial a_1^2} = h_1^2(1-h_1^2)\n",
    "$\n",
    "\n",
    "Then compute,\n",
    "$\n",
    "\\frac{\\partial a_1^2}{\\partial b^2} = 1\n",
    "$\n",
    "\n",
    "We know that,\n",
    "\n",
    "$\n",
    "h_1^2 = \\hat{y_1}\n",
    "$\n",
    "\n",
    "So, apply all the above computed values here to get,\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial b^2} = ({\\hat{y_1} - y_1})\n",
    "$\n",
    "\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard Python libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbcbdd4cf10>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "def setParameters(X, Y, hidden_size):\n",
    "  np.random.seed(3)\n",
    "  input_size = X.shape[0] # number of neurons in input layer\n",
    "  output_size = Y.shape[0] # number of neurons in output layer.\n",
    "  W1 = np.random.randn(hidden_size, input_size)*np.sqrt(2/input_size)\n",
    "  b1 = np.zeros((hidden_size, 1))\n",
    "  W2 = np.random.randn(output_size, hidden_size)*np.sqrt(2/hidden_size)\n",
    "  b2 = np.zeros((output_size, 1))\n",
    "  return {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}\n",
    "def forwardPropagation(X, params):\n",
    "  Z1 = np.dot(params['W1'], X)+params['b1']\n",
    "  A1 = np.tanh(Z1)\n",
    "  Z2 = np.dot(params['W2'], A1)+params['b2']\n",
    "  y = sigmoid(Z2)  \n",
    "  return y, {'Z1': Z1, 'Z2': Z2, 'A1': A1, 'y': y}\n",
    "def cost(predict, actual):\n",
    "  m = actual.shape[1]\n",
    "# binary cross entropy loss function\n",
    "  cost__ = -np.sum(np.multiply(np.log(predict), actual) + np.multiply((1 - actual), np.log(1 - predict)))/m\n",
    "  return np.squeeze(cost__)\n",
    "def backPropagation(X, Y, params, cache):\n",
    "  m = X.shape[1]\n",
    "  dy = cache['y'] - Y\n",
    "  dW2 = (1 / m) * np.dot(dy, np.transpose(cache['A1']))\n",
    "  db2 = (1 / m) * np.sum(dy, axis=1, keepdims=True)\n",
    "  dZ1 = np.dot(np.transpose(params['W2']), dy) * (1-np.power(cache['A1'], 2))\n",
    "  dW1 = (1 / m) * np.dot(dZ1, np.transpose(X))\n",
    "  db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "  return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "# Optimize the parameters\n",
    "def updateParameters(gradients, params, learning_rate = 1.2):\n",
    "    W1 = params['W1'] - learning_rate * gradients['dW1']\n",
    "    b1 = params['b1'] - learning_rate * gradients['db1']\n",
    "    W2 = params['W2'] - learning_rate * gradients['dW2']\n",
    "    b2 = params['b2'] - learning_rate * gradients['db2']\n",
    "    return {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}\n",
    "def fit(X, Y, learning_rate, hidden_size, number_of_iterations = 5000):\n",
    "  params = setParameters(X, Y, hidden_size)\n",
    "  cost_ = []\n",
    "  for j in range(number_of_iterations):\n",
    "    y, cache = forwardPropagation(X, params)\n",
    "    costit = cost(y, Y)\n",
    "    gradients = backPropagation(X, Y, params, cache)\n",
    "    params = updateParameters(gradients, params, learning_rate)\n",
    "    cost_.append(costit)\n",
    "  return params, cost_\n",
    "def predict(X, parameters):\n",
    "  a2,var = forwardPropagation(X, parameters)\n",
    "  yhat = a2\n",
    "  yhat = np.squeeze(yhat)\n",
    "  if(yhat >= 0.5).all():\n",
    "    y_predict = 1\n",
    "  else:\n",
    "    y_predict = 0\n",
    "\n",
    "  return(y_predict)\n",
    "\n",
    "# The 4 training examples by columns\n",
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "\n",
    "# The outputs of the XOR for every example in X\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "# No. of training examples\n",
    "m = X.shape[1]\n",
    "\n",
    "num_of_iters = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "trained_parameters, cost_ = fit(X, Y, learning_rate,num_of_iters)\n",
    "\n",
    "print('Accuracy: {}'.format(np.mean(Y==predict(X,trained_parameters))))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(cost_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
